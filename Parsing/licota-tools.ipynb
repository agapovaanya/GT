{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import urllib \n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import _datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://licota-tools.ru/'\n",
    "r = requests.get(url)\n",
    "html_content = r.text\n",
    "soup = BeautifulSoup(html_content,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(soup.findAll('li'))):\n",
    "    if soup.findAll('li')[i].find('a')!=None:\n",
    "        lin=soup.findAll('li')[i].find('a').get('href')\n",
    "        category.append(lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_l=[]\n",
    "for i in range(len(soup.findAll('a',class_='image'))):\n",
    "    category_l.append(soup.findAll('a',class_='image')[i].get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_fin=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(category_l)):\n",
    "    for i in range(len(category)):\n",
    "        if category_l[j] in category[i]:\n",
    "            category_fin.append(category[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_fin=np.unique(category_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcategory=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in range(len(category_fin)):\n",
    "    url = category_fin[u]\n",
    "    r = requests.get(url)\n",
    "    html_content = r.text\n",
    "    soup = BeautifulSoup(html_content,'html.parser')\n",
    "    if soup.findAll('div',class_='image')!=[]:\n",
    "        pod=soup.findAll('div',class_='image')\n",
    "        for y in range(len(pod)):\n",
    "            a=soup.findAll('div',class_='image')[y].find('a').get('href')\n",
    "            if a not in category_fin:\n",
    "                podcategory.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(podcategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in range(len(podcategory)):\n",
    "    url = podcategory[u]\n",
    "    r = requests.get(url)\n",
    "    html_content = r.text\n",
    "    soup = BeautifulSoup(html_content,'html.parser')\n",
    "    if soup.findAll('div',class_='image')!=[]:\n",
    "        pod=soup.findAll('div',class_='image')\n",
    "        for y in range(len(pod)):\n",
    "            a=soup.findAll('div',class_='image')[y].find('a').get('href')\n",
    "            if a not in category_fin and a not in podcategory:\n",
    "                kk.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_category=list(category_fin)+kk+podcategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linki=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in range(len(all_category)):\n",
    "    print(o)\n",
    "    url = all_category[o]\n",
    "    r = requests.get(url)\n",
    "    html_content = r.text\n",
    "    soup = BeautifulSoup(html_content,'html.parser')\n",
    "    if soup.findAll('div',class_='results')!=[]:\n",
    "        st=soup.findAll('div',class_='results')[0].text\n",
    "        beg=st.find('всего ')\n",
    "        end=st.find('страниц')\n",
    "        page=int(st[beg+6:end-1])\n",
    "        if page!=1:\n",
    "            for i in range(page):\n",
    "                url1=url+'?page='+str(i)\n",
    "                r1 = requests.get(url1)\n",
    "                html_content1 = r1.text\n",
    "                soup1 = BeautifulSoup(html_content1,'html.parser')\n",
    "                if soup1.findAll('div',class_='name')!=[]:\n",
    "                    gg=soup1.findAll('div',class_='name')\n",
    "                    for r in range(len(gg)):\n",
    "                        linki.append(gg[r].find('a').get('href'))\n",
    "        else:\n",
    "            gg=soup.findAll('div',class_='name')\n",
    "            for r in range(len(gg)):\n",
    "                linki.append(gg[r].find('a').get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linki=np.unique(linki)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_all=[]\n",
    "brand_all=[]\n",
    "price_all=[]\n",
    "article_all=[]\n",
    "linki_all=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(linki)):\n",
    "    print(t)\n",
    "    article=''\n",
    "    name=''\n",
    "    brand=''\n",
    "    price=''\n",
    "    url = linki[t]\n",
    "    r = requests.get(url)\n",
    "    html_content = r.text\n",
    "    soup = BeautifulSoup(html_content,'html.parser')\n",
    "    name=soup.findAll('h1')[0].text\n",
    "    if soup.findAll('div',class_='description')!=[] and soup.findAll('div',class_='description')[0].find('a')!=None:\n",
    "        brand=soup.findAll('div',class_='description')[0].find('a').text\n",
    "        beg=soup.findAll('div',class_='description')[0].text.find('Модель')\n",
    "        end=soup.findAll('div',class_='description')[0].text.find('Наличие')\n",
    "        article=soup.findAll('div',class_='description')[0].text[beg+8:end-1]\n",
    "        price=soup.findAll('div',class_='right')[0].find('div',class_='price').text\n",
    "    name_all.append(name)\n",
    "    brand_all.append(brand)\n",
    "    price_all.append(price)\n",
    "    article_all.append(article)\n",
    "    linki_all.append(linki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame(list(zip(name_all, brand_all,article_all,price_all,linki)), columns =['Наименование товара', 'Бренд', 'Артикул','Цена','Ссылка на товар']) \n",
    "\n",
    "table.to_excel('licota-tools300721.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
