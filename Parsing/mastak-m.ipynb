{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import urllib \n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import _datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_datetime1 = datetime.datetime.now()\n",
    "temporary_links = []\n",
    "primary_links = []\n",
    "product_links = []\n",
    "titles = []\n",
    "prices = []\n",
    "marks=[]\n",
    "articles=[]\n",
    "valutes=[]\n",
    "categories=[]\n",
    "k=0\n",
    "m=0\n",
    "url = 'https://mactak-m.ru/catalog/'\n",
    "r = requests.get(url)\n",
    "html_content = r.text\n",
    "soup = BeautifulSoup(html_content,'html.parser')\n",
    "\n",
    "for link in soup.findAll('a', attrs={'href': re.compile(\"/catalog/\")},class_=\"group-link\"):\n",
    "    temporary_links.append(link.get('href'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in np.unique(temporary_links):\n",
    "    url = 'https://mactak-m.ru' + link\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        r.status_code = \"Connection refused\"\n",
    "        print(\"pobeda\")\n",
    "    html_content = r.text\n",
    "    soup = BeautifulSoup(html_content,'html.parser')\n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(\"/catalog/\")},class_=\"group-link\"):\n",
    "                primary_links.append(link.get('href'))\n",
    "    #if soup.findAll('a', attrs={'href': re.compile(\"/catalog/\")},class_=\"catalog-item\")==[]:\n",
    "        #primary_links.append(link)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(10):\n",
    "    print(j)\n",
    "    if j==0:\n",
    "        ui=0\n",
    "    else:\n",
    "        ui=p\n",
    "    print(p,\"1\")\n",
    "    p=len(primary_links)\n",
    "    print(p,'2')\n",
    "    #m=0\n",
    "    for i in range(ui,len(primary_links)):\n",
    "        \n",
    "        m=m+1\n",
    "        print(i,m)\n",
    "        url = 'https://mactak-m.ru' + primary_links[i]\n",
    "        r = requests.get(url)\n",
    "        html_content = r.text\n",
    "        soup = BeautifulSoup(html_content,'html.parser')\n",
    "        #temporary_links = []\n",
    "\n",
    "        if soup.findAll('a', attrs={'href': re.compile(\"/catalog/\")},class_=\"group-link\")!=[]:\n",
    "            print('stop')\n",
    "            for link in soup.findAll('a', attrs={'href': re.compile(\"/catalog/\")},class_=\"group-link\"):\n",
    "                primary_links.append(link.get('href'))\n",
    "                \n",
    "\n",
    "print(len(primary_links))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_links=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(173,len(primary_links)):\n",
    "    print(i)\n",
    "    url='https://mactak-m.ru'+primary_links[i]\n",
    "    r = requests.get(url)\n",
    "    html_content = r.text\n",
    "    soup = BeautifulSoup(html_content,'html.parser')\n",
    "    hrefs=[]\n",
    "    o=soup.findAll('a')\n",
    "    for t in range(len(o)):\n",
    "        if o[t].get('href')!=None and o[t].get('href').find('PAGEN')>0:\n",
    "            hrefs.append(o[t].get('href'))\n",
    "    if hrefs==[]:\n",
    "        max_pages=1\n",
    "    else:\n",
    "        max_pages=int(hrefs[-1][-1])\n",
    "        \n",
    "        \n",
    "    if max_pages>1:\n",
    "        \n",
    "        for g in range(1,max_pages+1):\n",
    "        #разделение на несколько страниц\n",
    "            url='https://mactak-m.ru'+primary_links[i]+'?PAGEN_1='+str(g)\n",
    "            r = requests.get(url)\n",
    "            html_content = r.text\n",
    "            soup = BeautifulSoup(html_content,'html.parser')\n",
    "            for ul in soup.findAll('a',title=True,href=True):\n",
    "                product_links.append(ul.get('href'))\n",
    "\n",
    "    else:\n",
    "        for u in soup.findAll('a',title=True,href=True):\n",
    "            product_links.append(u.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_links=np.unique(product_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(product_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb=xlrd.open_workbook('/Users/anna/Downloads/прогноз_факт_шаблон.xlsx') \n",
    "sheet=rb.sheet_by_index(0) \n",
    "fact=[] \n",
    "for rownum in range(sheet.nrows): \n",
    "    row=sheet.row_values(rownum) \n",
    "    for c_el in row: \n",
    "        fact.append(c_el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import _datetime\n",
    "import numpy as np\n",
    "import math\n",
    "import xlrd\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb=xlrd.open_workbook('/Users/anna/Downloads/link mactak.xlsx') \n",
    "sheet=rb.sheet_by_index(0) \n",
    "fact=[] \n",
    "for rownum in range(sheet.nrows): \n",
    "    row=sheet.row_values(rownum) \n",
    "    for c_el in row: \n",
    "        fact.append(c_el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_final=[]\n",
    "proizvoditel_final=[]\n",
    "price_final=[]\n",
    "kroshki_final=[]\n",
    "name_final=[]\n",
    "url_final=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for k in range(len(fact)):\n",
    "    print(k,name)\n",
    "    for j in range(2):\n",
    "        #time.sleep(1)\n",
    "        url=fact[k]\n",
    "        r = requests.get(url)\n",
    "\n",
    "        html_content = r.text.encode('ISO-8859-1','ignore').decode('utf-8','ignore')\n",
    "\n",
    "        soup = BeautifulSoup(html_content,'html.parser')\n",
    "\n",
    "        if soup.findAll('li',class_=\"item-ARTICLE\")==[]:\n",
    "            article=''\n",
    "        else:\n",
    "            article=soup.findAll('li',class_=\"item-ARTICLE\")[0].text.split()[-1]\n",
    "\n",
    "        if soup.findAll('h1',class_=\"cod\")==[]:\n",
    "            name=''\n",
    "        else:\n",
    "            name=soup.findAll('h1',class_=\"cod\")[0].text\n",
    "        \n",
    "\n",
    "        if soup.findAll('li',class_=\"item-PROIZVODITEL_NORMALIZE\")==[]:\n",
    "            proizvoditel=''\n",
    "        else:\n",
    "            proizvoditel=soup.findAll('li',class_=\"item-PROIZVODITEL_NORMALIZE\")[0].text[16:]\n",
    "            #kroshki=soup.findAll('div',class_=\"breadcrumbs\")[0].text.split()\n",
    "        if soup.findAll('div',class_=\"price\")==[]:\n",
    "            price=''\n",
    "        else:\n",
    "            price=soup.findAll('div',class_=\"price\")[0].text\n",
    "        kroshki=[]\n",
    "        for i in soup.findAll('a', attrs={'href': re.compile(\"/catalog/\")},title=True):\n",
    "            kroshki.append(i.get('title'))\n",
    "\n",
    "    article_final.append(article)\n",
    "    proizvoditel_final.append(proizvoditel)\n",
    "    price_final.append(price)\n",
    "    kroshki_final.append(kroshki)\n",
    "    name_final.append(name)\n",
    "    url_final.append(url)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame(list(zip(article_final, name_final,proizvoditel_final, price_final,kroshki_final,url_final))) \n",
    "\n",
    "table.to_excel('mactak-m130321_12.xlsx')\n",
    "\n",
    "current_datetime2 = datetime.datetime.now()\n",
    "print (current_datetime2 - current_datetime1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
