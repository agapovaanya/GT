{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import urllib \n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import _datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_datetime1 = datetime.datetime.now()\n",
    "temporary_links = []\n",
    "primary_links = []\n",
    "product_links = []\n",
    "titles = []\n",
    "prices = []\n",
    "marks=[]\n",
    "articles=[]\n",
    "valutes=[]\n",
    "categories=[]\n",
    "k=0\n",
    "m=0\n",
    "url = 'https://favorit-tools.ru/'\n",
    "r = requests.get(url)\n",
    "html_content = r.text\n",
    "soup = BeautifulSoup(html_content,'html.parser')\n",
    "\n",
    "for link in soup.findAll('a',class_=\"categories-v__link categories-v__link--with-subs\"):\n",
    "    temporary_links.append(link.get('href'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in np.unique(temporary_links):\n",
    "    url = 'https://favorit-tools.ru' + link\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        r.status_code = \"Connection refused\"\n",
    "        print(\"pobeda\")\n",
    "    html_content = r.text\n",
    "    soup = BeautifulSoup(html_content,'html.parser')\n",
    "    for link in soup.findAll('a',class_=\"categories-icons__inner js-category-link\"):\n",
    "                primary_links.append(link.get('href'))\n",
    "    #if soup.findAll('a', attrs={'href': re.compile(\"/catalog/\")},class_=\"catalog-item\")==[]:\n",
    "        #primary_links.append(link)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkk=[]\n",
    "for link in np.unique(primary_links):\n",
    "    url = 'https://favorit-tools.ru' + link\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        r.status_code = \"Connection refused\"\n",
    "        print(\"pobeda\")\n",
    "    html_content = r.text\n",
    "    soup = BeautifulSoup(html_content,'html.parser')\n",
    "    if soup.findAll('a',class_=\"categories-icons__inner js-category-link\")==[]:\n",
    "        kkk.append(link)\n",
    "    else:\n",
    "        for link in soup.findAll('a',class_=\"categories-icons__inner js-category-link\"):\n",
    "                kkk.append(link.get('href'))\n",
    "    #if soup.findAll('a', attrs={'href': re.compile(\"/catalog/\")},class_=\"catalog-item\")==[]:\n",
    "        #primary_links.append(link)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(kkk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_prefinal=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(kkk)):\n",
    "    print(i)\n",
    "    url = 'https://favorit-tools.ru'+kkk[i]\n",
    "    r = requests.get(url)\n",
    "    html_content = r.text\n",
    "    soup = BeautifulSoup(html_content,'html.parser')\n",
    "    if soup.findAll('a', attrs={'href': re.compile(\"/?page=\")},class_=False)==[]:\n",
    "        all_link_product=soup.findAll('div', class_=\"product-tile__name\")\n",
    "        for y in range(len(all_link_product)):\n",
    "            nach=str(soup.findAll('div', class_=\"product-tile__name\")[y]).find('href=')+6\n",
    "            fin=str(soup.findAll('div', class_=\"product-tile__name\")[y])[nach:].find('/\"')\n",
    "            linki=str(soup.findAll('div', class_=\"product-tile__name\")[y])[nach:fin+nach]\n",
    "            link_prefinal.append(linki)\n",
    "    else:\n",
    "        max_page=int(soup.findAll('a', attrs={'href': re.compile(\"/?page=\")},class_=False)[-1].text)\n",
    "        for u in range(max_page):\n",
    "            url='https://favorit-tools.ru'+kkk[i]+'?page='+str(u)\n",
    "            r = requests.get(url)\n",
    "            html_content = r.text\n",
    "            soup = BeautifulSoup(html_content,'html.parser')\n",
    "            all_link_product=soup.findAll('div', class_=\"product-tile__name\")\n",
    "            for y in range(len(all_link_product)):\n",
    "                nach=str(soup.findAll('div', class_=\"product-tile__name\")[y]).find('href=')+6\n",
    "                fin=str(soup.findAll('div', class_=\"product-tile__name\")[y])[nach:].find('/\"')\n",
    "                linki=str(soup.findAll('div', class_=\"product-tile__name\")[y])[nach:fin+nach]\n",
    "                link_prefinal.append(linki)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_prefinal=np.unique(link_prefinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_final=[]\n",
    "proizvoditel_final=[]\n",
    "price_final=[]\n",
    "kroshki_final=[]\n",
    "name_final=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for k in range(len(link_prefinal)):\n",
    "    print(k)\n",
    "    url='https://favorit-tools.ru'+link_prefinal[k]\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        r.status_code = \"Connection refused\"\n",
    "        print('alarm')\n",
    "    html_content = r.text\n",
    "    soup = BeautifulSoup(html_content,'html.parser')\n",
    "\n",
    "    if soup.findAll('span',itemprop=\"sku\")==[]:\n",
    "        article=''\n",
    "    else:\n",
    "        article=soup.findAll('span',itemprop=\"sku\")[0].text\n",
    "    \n",
    "    if soup.findAll('h1')==[]:\n",
    "        name=''\n",
    "    else:\n",
    "        name=soup.findAll('h1')[0].text\n",
    "    \n",
    "    \n",
    "    if soup.findAll('td',class_=\"product_features-value\",itemprop=\"brand\")==[]:\n",
    "        proizvoditel=''\n",
    "    else:\n",
    "        proizvoditel=soup.findAll('td',class_=\"product_features-value\",itemprop=\"brand\")[0].text\n",
    "    #kroshki=soup.findAll('div',class_=\"breadcrumbs\")[0].text.split()\n",
    "    if soup.findAll('div',class_=\"price\")==[]:\n",
    "        price=''\n",
    "    else:\n",
    "        price=soup.findAll('div',class_=\"price\")[0].text\n",
    "    \n",
    "    \n",
    "    kroshki=[]\n",
    "    for i in soup.findAll('span',itemprop=\"name\"):\n",
    "        kroshki.append(i.text)\n",
    "    \n",
    "    article_final.append(article)\n",
    "    proizvoditel_final.append(proizvoditel)\n",
    "    price_final.append(price)\n",
    "    kroshki_final.append(kroshki)\n",
    "    name_final.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame(list(zip(article_final, name_final,proizvoditel_final, price_final,kroshki_final,link_prefinal))) \n",
    "\n",
    "table.to_excel('favorit-tools030621.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
