{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import urllib \n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import _datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_datetime1 = datetime.datetime.now()\n",
    "temporary_links = []\n",
    "primary_links = []\n",
    "product_links = []\n",
    "titles = []\n",
    "prices = []\n",
    "marks=[]\n",
    "articles=[]\n",
    "valutes=[]\n",
    "categories=[]\n",
    "k=0\n",
    "m=0\n",
    "url = 'https://arstools.ru'\n",
    "r = requests.get(url)\n",
    "html_content = r.text\n",
    "soup = BeautifulSoup(html_content,'html.parser')\n",
    "\n",
    "for link in soup.findAll('a',class_=\"dropdown-toggle\"):\n",
    "    temporary_links.append(link.get('href'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkk=[]\n",
    "for t in temporary_links[1:]:\n",
    "    url =t\n",
    "    r = requests.get(url)\n",
    "    html_content = r.text\n",
    "    soup = BeautifulSoup(html_content,'html.parser')\n",
    "    for k in soup.findAll('div',class_=\"well well-sm\"):\n",
    "        nach=str(k).find('well-sm\"><a href=\"')\n",
    "\n",
    "\n",
    "        kon=str(k)[nach+18:].find('><')\n",
    "\n",
    "        link=str(k)[nach+18:nach+17+kon]\n",
    "        kkk.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(kkk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lll=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for t in range(len(kkk)):\n",
    "    url =kkk[t]\n",
    "    r = requests.get(url)\n",
    "    html_content = r.text\n",
    "    soup = BeautifulSoup(html_content,'html.parser')\n",
    "    for k in soup.findAll('div',class_=\"well well-sm\"):\n",
    "        nach=str(k).find('well-sm\"><a href=\"')\n",
    "\n",
    "\n",
    "        kon=str(k)[nach+18:].find('><')\n",
    "\n",
    "        link=str(k)[nach+18:nach+17+kon]\n",
    "        lll.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_fin=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for u in range(len(lll)):\n",
    "    print(u)\n",
    "    url =lll[u]\n",
    "    r = requests.get(url)\n",
    "    html_content = r.text\n",
    "    soup = BeautifulSoup(html_content,'html.parser')\n",
    "    nach=str(soup.findAll('form',id=\"sorts\")).find('action=\"')+8\n",
    "    kon=str(soup.findAll('form',id=\"sorts\"))[nach:].find('\"')+nach\n",
    "    link=str(soup.findAll('form',id=\"sorts\"))[nach:kon]\n",
    "    if soup.findAll('ul',class_=\"pagination pull-right\")==[]:\n",
    "        max_page=1\n",
    "    else:\n",
    "        if soup.findAll('ul',class_=\"pagination pull-right\")[-1].text.find('...')>-1:\n",
    "            max_page=soup.findAll('ul',class_=\"pagination pull-right\")[-1].text[-3:-1]\n",
    "        else:\n",
    "            max_page=soup.findAll('ul',class_=\"pagination pull-right\")[-1].text[-2]\n",
    "\n",
    "    \n",
    "    for i in range(int(max_page)):\n",
    "        print(i)\n",
    "        if i==0:\n",
    "            link_page=link\n",
    "        else:\n",
    "            link_page=link+':'+str(20*i)\n",
    "\n",
    "        r = requests.get(link_page)\n",
    "        html_content = r.text\n",
    "        soup = BeautifulSoup(html_content,'html.parser')\n",
    "        for url_link in soup.findAll('a',itemprop=\"url\"):\n",
    "            links_fin.append(url_link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_fin=np.unique(links_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_final=[]\n",
    "article_final=[]\n",
    "kroshki_final=[]\n",
    "price_final=[]\n",
    "proizv_final=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for ii in range(1885,len(links_fin)):\n",
    "    print(ii)\n",
    "    r = requests.get(links_fin[ii])\n",
    "    html_content = r.text\n",
    "    soup = BeautifulSoup(html_content,'html.parser')\n",
    "\n",
    "    if soup.findAll('h1',itemprop=\"name\")==[]:\n",
    "        name=''\n",
    "    else:\n",
    "        name=soup.findAll('h1',itemprop=\"name\")[0].text\n",
    "\n",
    "    article=''\n",
    "    proizv=''\n",
    "    if soup.findAll('div',class_=\"col-sm-6\")!=[]:\n",
    "\n",
    "        for j in soup.findAll('div',class_=\"col-sm-6\"):\n",
    "            if j.text.find('Артикул')>-1:\n",
    "                article=j.text[9:]\n",
    "            else:\n",
    "                if j.text.find('Производитель')>-1:\n",
    "                    proizv=j.text[15:]\n",
    "    if soup.findAll('span',class_=\"bk_price\")==[]:\n",
    "        price=''\n",
    "    else:\n",
    "\n",
    "        price=soup.findAll('span',class_=\"bk_price\")[0].text\n",
    "\n",
    "    kroshki=[]\n",
    "    if soup.findAll('span',itemprop=\"name\")!=[]:\n",
    "        for y in soup.findAll('span',itemprop=\"name\"):\n",
    "            kroshki.append(y.text)\n",
    "\n",
    "    name_final.append(name)\n",
    "    article_final.append(article)\n",
    "    kroshki_final.append(kroshki)\n",
    "    price_final.append(price)\n",
    "    proizv_final.append(proizv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame(list(zip(article_final, name_final,proizv_final, price_final,kroshki_final,links_fin))) \n",
    "\n",
    "table.to_excel('arstools030621.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
