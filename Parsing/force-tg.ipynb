{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import urllib \n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporary_links = []\n",
    "primary_links = []\n",
    "product_links = []\n",
    "titles = []\n",
    "prices = []\n",
    "marks=[]\n",
    "articles=[]\n",
    "valutes=[]\n",
    "categories=[]\n",
    "tags=[]\n",
    "links=[]\n",
    "primary_links1=[]\n",
    "primary_links2=[]\n",
    "url='https://force-tg.ru/store/'\n",
    "r = requests.get(url)\n",
    "html_content = r.text\n",
    "soup = BeautifulSoup(html_content,'html.parser')\n",
    "for link in soup.findAll('a', attrs={'href': re.compile(\"https://force-tg.ru/store/\")}):\n",
    "    temporary_links.append(link.get('href'))\n",
    "primary_links=np.unique(temporary_links[:-12])\n",
    "for primary_link in primary_links:\n",
    "    print(primary_link)\n",
    "    url=primary_link\n",
    "    r = requests.get(url)\n",
    "    html_content = r.text\n",
    "    soup = BeautifulSoup(html_content,'html.parser')\n",
    "    nbnb=[]\n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(\"https://force-tg.ru/store/\"),'title':not 'Распродажа'},class_=[]):\n",
    "        nbnb.append(link.get('href'))\n",
    "    ko=np.unique(nbnb)\n",
    "    mmmm=[]\n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(\"https://force-tg.ru/store/\"),'title': not None},class_=[]):\n",
    "        mmmm.append(link.get('href')) \n",
    "    ooo=np.unique(mmmm)\n",
    "    mom=list(ko)\n",
    "    ttt=[]\n",
    "    for i in range(len(ko)):\n",
    "        for j in range(len(ooo)):\n",
    "            if ko[i]==ooo[j]:\n",
    "                ttt.append(i)\n",
    "\n",
    "    for i in range(1,len(ttt)+1):\n",
    "        del mom[int(ttt[-i])]\n",
    "    primary_links=list(primary_links)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    if mom==[]:\n",
    "        url=primary_link\n",
    "        r = requests.get(url)\n",
    "        html_content = r.text\n",
    "        soup = BeautifulSoup(html_content,'html.parser')\n",
    "        for i in soup.findAll('a', attrs={'title':'Подробное описание товара'}):\n",
    "            product_links.append(i.get('href'))\n",
    "    else:\n",
    "        for i in np.unique(list(mom)):\n",
    "            primary_links.append(i)\n",
    "            \n",
    "    \n",
    " \n",
    "\n",
    "y=0\n",
    "for t in primary_links:\n",
    "    url=t\n",
    "    r = requests.get(url)\n",
    "    html_content = r.text\n",
    "    soup = BeautifulSoup(html_content,'html.parser')\n",
    "    if soup.findAll('div',class_=\"uss_shop_cat_name\")==[]:\n",
    "        b=8\n",
    "    else:\n",
    "        for i in soup.findAll('div',class_=\"uss_shop_cat_name\"):\n",
    "            nac=str(i('a')).find('href')\n",
    "            con=str(i('a')).find('/\">')\n",
    "            linn=str(i('a'))[nac+6:con+1]\n",
    "            primary_links.append(linn)\n",
    "            \n",
    "    if soup.findAll('span',class_=\"uss_page uss_last\")==[]:\n",
    "        last_pages=1\n",
    "    else:    \n",
    "        last_pages=int(soup.findAll('span',class_=\"uss_page uss_last\")[0].text)\n",
    "    for page in range(1,last_pages+1):\n",
    "        urli=url+'?page='+str(page)\n",
    "        r = requests.get(urli)\n",
    "        html_content = r.text\n",
    "        soup = BeautifulSoup(html_content,'html.parser')\n",
    "        \n",
    "        for i in soup.findAll('a', attrs={'title':'Подробное описание товара'}):\n",
    "            product_links.append(i.get('href'))\n",
    "    #y+=1\n",
    "    #print(y)\n",
    "    \n",
    "    \n",
    "\n",
    "product_links_unique=np.unique(product_links)\n",
    "ggg=0\n",
    "for product_link in product_links_unique:\n",
    "    url=product_link\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        r.status_code = \"Connection refused\"\n",
    "    \n",
    "    html_content = r.text\n",
    "    soup = BeautifulSoup(html_content,'html.parser')\n",
    "    \n",
    "    title = soup.findAll('div',class_=\"uss_shop_description\")[0].text\n",
    "    titles.append(title)\n",
    "    \n",
    "    tag = soup.findAll('div',class_=\"uss_shop_uid\")[0].text\n",
    "    tags.append(tag)\n",
    "   \n",
    "    price = soup.findAll('em',class_=\"price_class\")[0].text\n",
    "    prices.append(price)\n",
    "    \n",
    "    abni=soup.findAll('div',class_=\"l-speedbar\")[0].text\n",
    "    k=0\n",
    "    for i in range(1,len(abni)-1):\n",
    "        if (abni[i]=='/')&(abni[i-1]==\" \")&(abni[i+1]==\" \"):\n",
    "            k=k+1\n",
    "    for jj in range(1,k):\n",
    "        c=abni.find(' / ')\n",
    "        abni=abni[c+4:]\n",
    "    lop=abni.find(' / ')\n",
    "    category=abni[:lop-2]\n",
    "    categories.append(category)\n",
    "    \n",
    "    links.append(url)\n",
    "    ggg=ggg+1\n",
    "    print(ggg)\n",
    "\n",
    "table = pd.DataFrame(list(zip(titles, tags,prices, categories,links)), columns =['Наименование товара','Код', 'Цена, р.','Категория', 'Ссылка']) \n",
    "\n",
    "table.to_excel('force-tg 030820.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
